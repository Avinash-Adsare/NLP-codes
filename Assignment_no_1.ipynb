{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a77a1095",
      "metadata": {
        "id": "a77a1095"
      },
      "source": [
        "### Name : Avinash Adsare\n",
        "### Roll No : 43501\n",
        "### Sub - NLP\n",
        "### Assignment No - 1\n",
        "\n",
        "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library.\n",
        "Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1435f3bf",
      "metadata": {
        "id": "1435f3bf",
        "outputId": "ea483b52-8685-48d0-8d42-d87e5756c8eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Yuvraj\n",
            "[nltk_data]     Pardeshi\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer, TweetTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef4f5f0",
      "metadata": {
        "id": "bef4f5f0"
      },
      "outputs": [],
      "source": [
        "text = \"NLTK is a powerful library for natural language processing. It provides various tokenization techniques.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4fdbfe7",
      "metadata": {
        "id": "c4fdbfe7",
        "outputId": "ce0911d3-49d1-4f8b-8f12-85a2170948e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whitespace Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing.', 'It', 'provides', 'various', 'tokenization', 'techniques.']\n"
          ]
        }
      ],
      "source": [
        "# Whitespace Tokenization\n",
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
        "print(\"Whitespace Tokenization:\", whitespace_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64bc94ba",
      "metadata": {
        "id": "64bc94ba"
      },
      "outputs": [],
      "source": [
        "punc_text = 'Hope is the only thing stronger than fear! #Hope'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fcc7ad9",
      "metadata": {
        "id": "4fcc7ad9",
        "outputId": "50edc87d-bf08-46dd-f752-d1623e2548e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Punctuation-based Tokenization: ['Hope', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope']\n"
          ]
        }
      ],
      "source": [
        "# Punctuation-based Tokenization\n",
        "#This tokenizer splits the sentences into words based on whitespaces and punctuations.\n",
        "punctuation_tokens = nltk.word_tokenize(punc_text)\n",
        "print(\"Punctuation-based Tokenization:\", punctuation_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cb9b1c",
      "metadata": {
        "id": "33cb9b1c"
      },
      "outputs": [],
      "source": [
        "treebank_text = \"What you don't want to do to yourself, don't do to others...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f3783a",
      "metadata": {
        "id": "84f3783a",
        "outputId": "47f788d4-514f-4eac-9c40-8801423b72f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treebank Tokenization: ['What', 'you', 'do', \"n't\", 'want', 'to', 'do', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
          ]
        }
      ],
      "source": [
        "# Treebank Tokenization\n",
        "''' This tokenizer incorporates a variety of common rules for english word tokenization.\n",
        "    It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains\n",
        "    decimal numbers as a single token. Besides, it contains rules for English contractions.\n",
        "    For example ‚Äúdon‚Äôt‚Äù is tokenized as [‚Äúdo‚Äù, ‚Äún‚Äôt‚Äù].'''\n",
        "\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(treebank_text)\n",
        "print(\"Treebank Tokenization:\", treebank_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2c0b94",
      "metadata": {
        "id": "4b2c0b94"
      },
      "outputs": [],
      "source": [
        "tweet_text = \"Don't take cryptocurrency advice form people on TwitterüòÖü§ê\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb738c2",
      "metadata": {
        "id": "0eb738c2",
        "outputId": "47edf4bf-4883-4d64-f9fd-ce6b5b4a63fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet Tokenization: [\"Don't\", 'take', 'cryptocurrency', 'advice', 'form', 'people', 'on', 'Twitter', 'üòÖ', 'ü§ê']\n"
          ]
        }
      ],
      "source": [
        "# Tweet Tokenization\n",
        "'''When we want to apply tokenization in text data like tweets, the tokenizers mentioned above can‚Äôt produce practical\n",
        "    tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different\n",
        "    words if we need them for tasks like sentiment analysis.'''\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(tweet_text)\n",
        "print(\"Tweet Tokenization:\", tweet_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339fe2f4",
      "metadata": {
        "id": "339fe2f4"
      },
      "outputs": [],
      "source": [
        "MWE_text = 'MS Dhoni is the king of cricket'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbba747",
      "metadata": {
        "id": "5bbba747",
        "outputId": "efb73bcc-8601-4a39-d20c-95d0ad1ef1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['MS_Dhoni', 'is', 'the', 'king', 'of', 'cricket']\n"
          ]
        }
      ],
      "source": [
        "#MWE Tokenization\n",
        "''' NLTK‚Äôs multi-word expression tokenizer (MWETokenizer) provides a function add_mwe()\n",
        "    that allows the user to enter multiple word expressions before using the tokenizer on the text.\n",
        "    More simply, it can merge multi-word expressions into single tokens.'''\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('MS','Dhoni'))\n",
        "print(tokenizer.tokenize(word_tokenize(MWE_text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b0c8b5",
      "metadata": {
        "id": "c5b0c8b5",
        "outputId": "5984adee-7780-40ad-bcd7-ee8ee0e235e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porter Stemming: ['connect', 'connect', 'connect', 'connect', 'connect', 'connect', 'connect']\n"
          ]
        }
      ],
      "source": [
        "# Stemming using Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "porter_list = ['Connects','Connecting','Connections','Connected','Connection','Connectings','Connect']\n",
        "porter_stemmed_words = [porter_stemmer.stem(word) for word in porter_list]\n",
        "print(\"Porter Stemming:\", porter_stemmed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314c92e1",
      "metadata": {
        "id": "314c92e1",
        "outputId": "4370fd39-1157-4e2e-a846-a4edbe9f869c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snowball Stemming: ['generous', 'generat', 'generous', 'generat']\n"
          ]
        }
      ],
      "source": [
        "# Stemming using Snowball Stemmer\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "snowball_list = ['generous','generate','generously','generation']\n",
        "snowball_stemmed_words = [snowball_stemmer.stem(word) for word in snowball_list]\n",
        "print(\"Snowball Stemming:\", snowball_stemmed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4c2e02",
      "metadata": {
        "id": "2a4c2e02",
        "outputId": "3456a03c-23fb-44a0-a7bb-0e044b0733f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Word--            --Lemma--           \n",
            "program             program             \n",
            "programming         program             \n",
            "programer           programer           \n",
            "programs            program             \n",
            "programmed          program             \n"
          ]
        }
      ],
      "source": [
        "# Lemmatization using Wordnet\n",
        "\n",
        "# Initialize wordnet lemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# Example inflections to reduce\n",
        "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
        "\n",
        "# Perform lemmatization\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
        "for word in example_words:\n",
        "    print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29dd0508",
      "metadata": {
        "id": "29dd0508"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}