{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77a1095",
   "metadata": {},
   "source": [
    "### Name : Yuvraj Pardeshi\n",
    "### Roll No : 43548\n",
    "### Sub - NLP\n",
    "### Assignment No - 1\n",
    "\n",
    "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library. \n",
    "Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1435f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Yuvraj\n",
      "[nltk_data]     Pardeshi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer, TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef4f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK is a powerful library for natural language processing. It provides various tokenization techniques.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fdbfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing.', 'It', 'provides', 'various', 'tokenization', 'techniques.']\n"
     ]
    }
   ],
   "source": [
    "# Whitespace Tokenization\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bc94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_text = 'Hope is the only thing stronger than fear! #Hope'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fcc7ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based Tokenization: ['Hope', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation-based Tokenization\n",
    "#This tokenizer splits the sentences into words based on whitespaces and punctuations.\n",
    "punctuation_tokens = nltk.word_tokenize(punc_text)\n",
    "print(\"Punctuation-based Tokenization:\", punctuation_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cb9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_text = \"What you don't want to do to yourself, don't do to others...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f3783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization: ['What', 'you', 'do', \"n't\", 'want', 'to', 'do', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
     ]
    }
   ],
   "source": [
    "# Treebank Tokenization\n",
    "''' This tokenizer incorporates a variety of common rules for english word tokenization. \n",
    "    It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains\n",
    "    decimal numbers as a single token. Besides, it contains rules for English contractions.\n",
    "    For example ‚Äúdon‚Äôt‚Äù is tokenized as [‚Äúdo‚Äù, ‚Äún‚Äôt‚Äù].'''\n",
    "\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(treebank_text)\n",
    "print(\"Treebank Tokenization:\", treebank_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2c0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = \"Don't take cryptocurrency advice form people on TwitterüòÖü§ê\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb738c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenization: [\"Don't\", 'take', 'cryptocurrency', 'advice', 'form', 'people', 'on', 'Twitter', 'üòÖ', 'ü§ê']\n"
     ]
    }
   ],
   "source": [
    "# Tweet Tokenization\n",
    "'''When we want to apply tokenization in text data like tweets, the tokenizers mentioned above can‚Äôt produce practical \n",
    "    tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different \n",
    "    words if we need them for tasks like sentiment analysis.'''\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(tweet_text)\n",
    "print(\"Tweet Tokenization:\", tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MWE_text = 'MS Dhoni is the king of cricket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bbba747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MS_Dhoni', 'is', 'the', 'king', 'of', 'cricket']\n"
     ]
    }
   ],
   "source": [
    "#MWE Tokenization\n",
    "''' NLTK‚Äôs multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() \n",
    "    that allows the user to enter multiple word expressions before using the tokenizer on the text.\n",
    "    More simply, it can merge multi-word expressions into single tokens.'''\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('MS','Dhoni'))\n",
    "print(tokenizer.tokenize(word_tokenize(MWE_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5b0c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemming: ['connect', 'connect', 'connect', 'connect', 'connect', 'connect', 'connect']\n"
     ]
    }
   ],
   "source": [
    "# Stemming using Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_list = ['Connects','Connecting','Connections','Connected','Connection','Connectings','Connect']\n",
    "porter_stemmed_words = [porter_stemmer.stem(word) for word in porter_list]\n",
    "print(\"Porter Stemming:\", porter_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "314c92e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball Stemming: ['generous', 'generat', 'generous', 'generat']\n"
     ]
    }
   ],
   "source": [
    "# Stemming using Snowball Stemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "snowball_list = ['generous','generate','generously','generation']\n",
    "snowball_stemmed_words = [snowball_stemmer.stem(word) for word in snowball_list]\n",
    "print(\"Snowball Stemming:\", snowball_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a4c2e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           programer           \n",
      "programs            program             \n",
      "programmed          program             \n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using Wordnet\n",
    "\n",
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Example inflections to reduce\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "\n",
    "# Perform lemmatization\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Lemma--\"))\n",
    "for word in example_words:\n",
    "    print (\"{0:20}{1:20}\".format(word, wnl.lemmatize(word, pos=\"v\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd0508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
