{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9823d90",
      "metadata": {
        "id": "a9823d90",
        "outputId": "639ff824-8a61-48fe-fea2-e50bbf76361c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: The quick brown fox jumps over the lazy dog.\n",
            "Cleaned: quick brown fox jump lazy dog\n",
            "\n",
            "Original: This is a sample document for testing the bag of words function.\n",
            "Cleaned: sample document testing bag word function\n",
            "\n",
            "Original: NLTK provides useful useful tools for text preprocessing.\n",
            "Cleaned: nltk provides useful useful tool text preprocessing\n",
            "\n",
            "Original: Bag of words is a simple and effective method for text representation.\n",
            "Cleaned: bag word simple effective method text representation\n",
            "\n",
            "Original: Machine learning algorithms often use bag of words as input features.\n",
            "Cleaned: machine learning algorithm often use bag word input feature\n",
            "\n",
            "TF-IDF Representation:\n",
            "   algorithm       bag     brown  document       dog  effective   feature  \\\n",
            "0   0.000000  0.000000  0.408248  0.000000  0.408248   0.000000  0.000000   \n",
            "1   0.000000  0.302637  0.000000  0.451891  0.000000   0.000000  0.000000   \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
            "3   0.000000  0.284329  0.000000  0.000000  0.000000   0.424555  0.000000   \n",
            "4   0.355851  0.238318  0.000000  0.000000  0.000000   0.000000  0.355851   \n",
            "\n",
            "        fox  function     input  ...  representation    sample    simple  \\\n",
            "0  0.408248  0.000000  0.000000  ...        0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.451891  0.000000  ...        0.000000  0.451891  0.000000   \n",
            "2  0.000000  0.000000  0.000000  ...        0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.000000  ...        0.424555  0.000000  0.424555   \n",
            "4  0.000000  0.000000  0.355851  ...        0.000000  0.000000  0.000000   \n",
            "\n",
            "    testing      text      tool       use    useful      word  label  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000      2  \n",
            "1  0.451891  0.000000  0.000000  0.000000  0.000000  0.302637      3  \n",
            "2  0.000000  0.274304  0.339992  0.000000  0.679984  0.000000      1  \n",
            "3  0.000000  0.342528  0.000000  0.000000  0.000000  0.284329      1  \n",
            "4  0.000000  0.000000  0.000000  0.355851  0.000000  0.238318      0  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Label Encoder Mapping:\n",
            "        label  encoded_label\n",
            "0      animal              2\n",
            "1  sample doc              3\n",
            "2         NLP              1\n",
            "3         NLP              1\n",
            "4          ML              0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"This is a sample document for testing the bag of words function.\",\n",
        "    \"NLTK provides useful useful tools for text preprocessing.\",\n",
        "    \"Bag of words is a simple and effective method for text representation.\",\n",
        "    \"Machine learning algorithms often use bag of words as input features.\"\n",
        "]\n",
        "\n",
        "# Label Encoding\n",
        "labels = ['animal', 'sample doc', 'NLP', 'NLP', 'ML']\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    clean_text = ' '.join(tokens)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Preprocess documents\n",
        "clean_documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Create TF-IDF representations\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(clean_documents)\n",
        "\n",
        "# Convert TF-IDF matrix to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add labels to DataFrame\n",
        "tfidf_df['label'] = encoded_labels\n",
        "\n",
        "# Save outputs\n",
        "tfidf_df.to_csv('tfidf_representation.csv', index=False)\n",
        "label_encoder_mapping = pd.DataFrame({'label': labels, 'encoded_label': encoded_labels})\n",
        "label_encoder_mapping.to_csv('label_encoder_mapping.csv', index=False)\n",
        "\n",
        "# Output the preprocessed documents\n",
        "for doc, clean_doc in zip(documents, clean_documents):\n",
        "    print(f\"Original: {doc}\")\n",
        "    print(f\"Cleaned: {clean_doc}\")\n",
        "    print()\n",
        "\n",
        "# Display TF-IDF DataFrame\n",
        "print(\"TF-IDF Representation:\")\n",
        "print(tfidf_df)\n",
        "\n",
        "# Display Label Encoder Mapping\n",
        "print(\"Label Encoder Mapping:\")\n",
        "print(label_encoder_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3.Perform text cleaning, perform lemmatization (any method), remove stop words (any method),\n",
        "label encoding. Create representations using TF-IDF. Save outputs.\n",
        "\n",
        "ext Cleaning:\n",
        "\n",
        "Text cleaning is the process of preparing raw text data for analysis or processing by removing irrelevant characters, symbols, or formatting. It typically involves steps such as: Lowercasing: Converting all text to lowercase to ensure consistency. Removing punctuation: Eliminating punctuation marks like commas, periods, and quotation marks. Removing special characters: Getting rid of non-alphanumeric characters such as emojis or symbols. Removing numbers: Excluding numerical digits that may not contribute to the textual meaning. Handling whitespace: Normalizing spaces, tabs, or line breaks. Removing HTML tags: Stripping out HTML tags if the text includes web content. Correcting spelling: Optionally, correcting spelling errors using techniques like spell-checking. Text cleaning helps improve the quality and consistency of textual data for further analysis or modeling.\n",
        "\n",
        "Lemmatization:\n",
        "\n",
        "Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma. Unlike stemming, which simply chops off prefixes or suffixes to derive a root word (stem), lemmatization considers the context and morphological analysis to produce valid lemmas. For example, the lemma of \"running\" is \"run,\" and the lemma of \"better\" is also \"good.\" Lemmatization often requires a dictionary or lexicon to map words to their respective lemmas. It is useful in text normalization tasks where maintaining the integrity of words is important, such as in search engines or machine translation systems.\n",
        "\n",
        "Removing Stop Words:\n",
        "\n",
        "Stop words are commonly used words in natural language that are often filtered out during text processing because they are considered to have little or no semantic meaning. Examples of stop words include \"the,\" \"is,\" \"and,\" \"in,\" \"of,\" etc. Removing stop words helps reduce noise in text data and focuses attention on more meaningful words that carry important information. However, the list of stop words may vary depending on the specific application or language, and it may be necessary to customize the stop word list accordingly.\n",
        "\n",
        "Label Encoding:\n",
        "\n",
        "Label encoding is a process of converting categorical labels or classes into numerical representations. It is commonly used in machine learning algorithms that require numerical input, such as regression or classification models. Each unique label or class is assigned a unique integer value. Label encoding is straightforward and can be done using simple mapping or encoding schemes. However, it's essential to ensure that the numerical representations do not imply any ordinal relationship between the categories unless such a relationship exists. Label encoding is different from one-hot encoding, where each category is represented by a binary vector. In label encoding, the numerical values are ordinal, while in one-hot encoding, they are not."
      ],
      "metadata": {
        "id": "QR4HWT2ZT1f4"
      },
      "id": "QR4HWT2ZT1f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69aa509",
      "metadata": {
        "id": "b69aa509"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}