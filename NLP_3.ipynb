{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Perform text cleaning, perform lemmatization (any method), remove stop words (any method),\n",
    "# label encoding. Create representations using TF-IDF. Save outputs. \n",
    "\n",
    "Text Cleaning:\n",
    "\n",
    "Text cleaning is the process of preparing raw text data for analysis or processing by removing irrelevant characters, symbols, or formatting.\n",
    "It typically involves steps such as:\n",
    "Lowercasing: Converting all text to lowercase to ensure consistency.\n",
    "Removing punctuation: Eliminating punctuation marks like commas, periods, and quotation marks.\n",
    "Removing special characters: Getting rid of non-alphanumeric characters such as emojis or symbols.\n",
    "Removing numbers: Excluding numerical digits that may not contribute to the textual meaning.\n",
    "Handling whitespace: Normalizing spaces, tabs, or line breaks.\n",
    "Removing HTML tags: Stripping out HTML tags if the text includes web content.\n",
    "Correcting spelling: Optionally, correcting spelling errors using techniques like spell-checking.\n",
    "Text cleaning helps improve the quality and consistency of textual data for further analysis or modeling.\n",
    "\n",
    "Lemmatization:\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma.\n",
    "Unlike stemming, which simply chops off prefixes or suffixes to derive a root word (stem), lemmatization considers the context and morphological analysis to produce valid lemmas.\n",
    "For example, the lemma of \"running\" is \"run,\" and the lemma of \"better\" is also \"good.\"\n",
    "Lemmatization often requires a dictionary or lexicon to map words to their respective lemmas.\n",
    "It is useful in text normalization tasks where maintaining the integrity of words is important, such as in search engines or machine translation systems.\n",
    "\n",
    "Removing Stop Words:\n",
    "\n",
    "Stop words are commonly used words in natural language that are often filtered out during text processing because they are considered to have little or no semantic meaning.\n",
    "Examples of stop words include \"the,\" \"is,\" \"and,\" \"in,\" \"of,\" etc.\n",
    "Removing stop words helps reduce noise in text data and focuses attention on more meaningful words that carry important information.\n",
    "However, the list of stop words may vary depending on the specific application or language, and it may be necessary to customize the stop word list accordingly.\n",
    "\n",
    "Label Encoding:\n",
    "\n",
    "Label encoding is a process of converting categorical labels or classes into numerical representations.\n",
    "It is commonly used in machine learning algorithms that require numerical input, such as regression or classification models.\n",
    "Each unique label or class is assigned a unique integer value.\n",
    "Label encoding is straightforward and can be done using simple mapping or encoding schemes.\n",
    "However, it's essential to ensure that the numerical representations do not imply any ordinal relationship between the categories unless such a relationship exists.\n",
    "Label encoding is different from one-hot encoding, where each category is represented by a binary vector. In label encoding, the numerical values are ordinal, while in one-hot encoding, they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5127,
     "status": "ok",
     "timestamp": 1705571880699,
     "user": {
      "displayName": "Adwait Deshpande",
      "userId": "14863366921114492910"
     },
     "user_tz": -330
    },
    "id": "-WEoVkz5AKnU",
    "outputId": "b1339d65-969f-4ace-bd54-f9edf2db89a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample data\n",
    "data = {'Text': [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "],\n",
    "    'Label': ['A', 'B', 'C', 'A']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Text Cleaning and Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Cleaned_Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['Encoded_Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# TF-IDF Representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['Cleaned_Text'])\n",
    "\n",
    "# Save Outputs\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "with open('tfidf_matrix.pkl', 'wb') as tfidf_file:\n",
    "    pickle.dump(X_tfidf, tfidf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQFrB4pgAXQw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON2kTILAvjKbh2MNn9bBou",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
